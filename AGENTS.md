# AGENTS.md — Agent Bench Tasks

This repository is cloned as the agent's workspace for every benchmark run.
Each task is fully self-contained in its own directory.

```
<repo-root>/                    ← workspace root (runner clone location)
├── CODING/
│   ├── 001/
│   │   ├── task.yaml           # Task definition (id, prompt, verification, …)
│   │   ├── verify.py           # Verification script
│   │   ├── results/            # Runtime outputs generated by task runs
│   │   │   └── output.txt
│   │   └── data/
│   │       └── temperature.py  # Fixture: broken script for the agent to fix
│   ├── 002/
│   │   ├── task.yaml
│   │   ├── verify.py
│   │   └── data/
│   │       └── server.log      # Fixture: log file for the agent to parse
│   └── 003/
│       ├── task.yaml
│       ├── verify.py
│       └── data/
│           └── sales.csv       # Fixture: CSV for the agent to aggregate
├── TOOLS/
│   ├── 001/
│   │   ├── task.yaml
│   │   └── verify.py           # No fixture data needed
│   ├── 002/
│   │   ├── task.yaml
│   │   └── verify.py           # No fixture data needed
│   └── 003/
│       ├── task.yaml
│       ├── verify.py
│       └── data/
│           ├── settings.json   # Fixture: config file read by the agent
│           └── input/          # Fixture: directory of text files to inspect
│               ├── changelog.txt
│               ├── notes.txt
│               └── tasks.txt
├── WRITING/
│   ├── 001/  task.yaml + verify.py
│   ├── 002/  task.yaml + verify.py
│   └── 003/  task.yaml + verify.py
```

---

## Task directory convention

Every task lives in `<CATEGORY>/<NNN>/` and contains exactly:

| File        | Required    | Purpose                                          |
| ----------- | ----------- | ------------------------------------------------ |
| `task.yaml` | yes         | Task definition loaded by the benchmark runner   |
| `verify.py` | yes         | Verification script run after the agent finishes |
| `data/`     | when needed | Fixture files the agent reads as input           |
| `results/`  | when run    | Runtime output files generated during execution  |

**All paths** in `task.yaml` prompts and in `verify.py` are relative to the
task directory (`<CATEGORY>/<NNN>/`), because both the agent and `verify.py`
run with that task directory as `cwd`.

---

## Task YAML format

```yaml
id: CODING-001 # Uppercase, matches directory (CODING/001)
title: "Short human-readable title"
category: bug-fix # bug-fix | feature | refactor | tools
difficulty: easy # easy | medium | hard

prompt: |
  Instructions for the agent.
  Reference fixture files as data/<filename>.
  Reference output files as results/<filename>.

verification:
  timeout: 30 # seconds (default: 60)

permissions:
  mode: "dontAsk" # dontAsk | bypassPermissions | default
  write: true
  bash: true
  read: true
  web_fetch: false

metadata:
  tags:
    - example-tag
```

### Field reference

| Field                  | Required | Values                                             |
| ---------------------- | -------- | -------------------------------------------------- |
| `id`                   | yes      | `CATEGORY-NNN` — uppercase, matches directory name |
| `category`             | yes      | `bug-fix`, `feature`, `refactor`, `tools`          |
| `difficulty`           | yes      | `easy`, `medium`, `hard`                           |
| `verification.timeout` | no       | seconds, default `60`                              |
| `permissions.mode`     | no       | `dontAsk` auto-approves all prompts                |
| `max_iterations`       | no       | positive integer, caps agent loop                  |

---

## Verification script conventions

`verify.py` runs with the **task directory** as `cwd`, so all paths are
relative to that task directory.

### Rules

- Shebang: `#!/usr/bin/env python3`
- Print `PASS: <description>` on success, `FAIL: <reason>` on failure
- Exit `0` on pass, `1` on fail
- Read agent output from `results/<filename>`
- Read fixture data from `data/<filename>`
- Standard library only — no third-party dependencies

### Template

```python
#!/usr/bin/env python3
"""Verification script for <CATEGORY>-<NNN>: <title>.

Fixture: data/<filename>   (omit if no fixture)
"""
import os
import sys


def verify() -> bool:
    output_file = "results/output.txt"

    if not os.path.exists(output_file):
        print(f"FAIL: '{output_file}' does not exist")
        return False

    content = open(output_file).read().strip()
    if not content:
        print(f"FAIL: '{output_file}' is empty")
        return False

    # Add task-specific validation here
    print(f"PASS: {content}")
    return True


if __name__ == "__main__":
    sys.exit(0 if verify() else 1)
```

### Validation tips

- Always check file existence before reading
- Strip whitespace before comparing strings (`content.strip()`)
- Derive expected values from fixture files dynamically — do not hardcode them
- Provide specific failure messages explaining what was wrong and what was expected

---

## Adding a new task

1. Choose the next available number in the category: `CODING/004/`, `TOOLS/004/`, etc.
2. Create `task.yaml` and `verify.py` inside that directory.
3. If the task needs input data, add it under `data/`.
4. Reference all paths from the task directory in both files.
5. Update the fixture table in this file.

---

## Running a verification script directly

```bash
# Run inside each task directory
(cd CODING/001 && python3 verify.py)
(cd TOOLS/003 && python3 verify.py)
```

Scripts exit `0` on PASS and `1` on FAIL.
